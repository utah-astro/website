// Enter speaker information here. The format is:
//
// ["Date","Name","Affiliation","Title","Abstract","Day","Time"]
//
// In the Abstract field you must escape double quotes (\"). Some HTML
// is possible (like <br>, <b>, etc.).
//
// ** Edited to add color change for special day/time.
// ** If Day or Time field is not empty, special day/time
// ** is/are added in date column in red.
//
// 201208, add host column
// ******* Please follow the format below. 
// ******* IMPORTANT: 
// ******* All the information for one talk should be in a single line.
// Month[i]=["date","Speaker","Institution","Title","Abstract","Host","Special Time Notes","Special Time Notes"];
// *******


October[0]=["20","Paul Woodward","Minnesota","Integrating Simulation and Visualization in Stellar Hydrodynamics","I am studying brief events in stars that can profoundly affect their evolution, although they do not result in explosions, and they do not last more than a few days. These events occur deep in the stellar interior, so that they are observed mainly by the side effects of their nucleosynthesis. They depend upon an interplay of fluid instabilities, turbulence, and combustion of new nuclear fuel that is drawn into hot, burning regions from stably stratified layers of the star. I will describe the challenges to computation that these simulations involve. The fluid flow regime is similar to that of weather on the earth, with flow Mach numbers in a regime where explicit methods must work hard, but are still efficient. As with global climate studies, the need to carry the simulation forward for many dynamical times – literally millions of time steps – produces a special challenge in scaling the computation to large machines. Success in this effort means that we must monitor the calculation closely as it proceeds, since computer time is consumed very rapidly during a run. We are integrating the visualization of the flow into the running application, so that we can see what is happening fast enough and accurately enough to know when the simulation should go forward and when it should be stopped. We are also always seeking ways in which to make the simulation run faster, and one potential way is to move the entire calculation to GPUs and 32-bit precision. In this talk I will outline the goals of the science, lay out the computational challenges, and describe ways in which we are trying to meet these challenges with an integrated approach to simulation and visualization.","Chris Johnson","Friday 2pm","",""]


November[0]=["3","Kiri Wagstaff","JPL","Machine Learning at the Outer Limits: In Search of Fast Radio Bursts from Extragalactic Sources","I will discuss the use of machine learning to analyze large-volume streaming data from radio astronomy telescopes.  We have developed the V-FASTR system for real-time detection and classification of radio pulses to enable potential discoveries of new radio sources, such as pulsars, star forming regions, and others we haven’t yet categorized. V-FASTR detects unusual signals and then determines whether they are most likely to be caused by interference or artifacts, or whether they indicate signs of a new source. We have created and repeatedly improved a web portal interface that allows human reviewers to assess each detection.  Machine learning is a key component in reducing the review burden, and each annotation made by a reviewer feeds back in to re-train the random forest classifier.  This symbiotic relationship enables continual improvement in classifier performance and in human time saved.","Vivek Srikumar","Friday 2pm","",""]


January[0]=["12","Alex Szalay","JHU","Numerical Laboratories: the Road to Exascale","The talk will describe how science is changing as a result of the vast amounts of data we are collecting from gene sequencers to telescopes and supercomputers. This “Fourth Paradigm of Science”, predicted by Jim Gray, is moving at full speed, and is transforming one scientific area after another. The talk will present various examples on the similarities of the emerging new challenges and how this vision is realized by the scientific community. Scientists are increasingly limited by their ability to analyze the large amounts of complex data available. These data sets are generated not only by instruments but also computational experiments; the sizes of the largest numerical simulations are on par with data collected by instruments, crossing the petabyte threshold. The importance of large synthetic data sets is increasingly important, as scientists compare their experiments to reference simulations. All disciplines need a new “instrument for data” that can deal not only with large data sets but the cross product of large and diverse data sets. There are several multi-faceted challenges related to this conversion, e.g. how to move, visualize, analyze and in general interact with Petabytes of data. The talk will outline how all these will have to change as we move towards Exascale computers.","Chris Johnson","Friday","2pm","",""]

January[1]=["19","Local Faculty","","Lunch and Afternoon Seminars","Off campus get-together for SoC/Astro faculty.","","12-6pm","",""]

March[0]=["2","Jeff Regier","Berkeley","TBD","TBD","Kyle Dawson","Friday 2pm","",""]

March[1]=["30","Elise Jennings","Argonne","TBD","TBD","Kyle Dawson","Friday 2pm","",""]
